// aiService.ts - FREE MODELS ONLY

interface AIModelConfig {
  Â  apiKey: string;
  Â  baseUrl?: string;
  }
  
  interface ChatMessage {
  Â  role: 'user' | 'assistant' | 'system';
  Â  content: string;
  }
  
  interface AIResponse {
  Â  content: string;
  Â  model: string;
  Â  usage?: {
  Â  Â  promptTokens: number;
  Â  Â  completionTokens: number;
  Â  Â  totalTokens: number;
  Â  };
  }
  
  /* ------------------ Gemini Service (FREE) ------------------ */
  class GeminiService {
  Â  private apiKey: string;
  Â  private baseUrl: string;
  
  Â  constructor(config: AIModelConfig) {
  Â  Â  this.apiKey = config.apiKey;
  Â  Â  this.baseUrl = config.baseUrl || 'https://generativelanguage.googleapis.com/v1beta';
  Â  }
  
  Â  async chat(messages: ChatMessage[], model: string = 'gemini-1.5-flash'): Promise<AIResponse> {
  Â  Â  // Convert chat messages to Gemini format
  Â  Â  const contents = messages
  Â  Â  Â  .filter(m => m.role !== 'system')
  Â  Â  Â  .map(m => ({
  Â  Â  Â  Â  role: m.role === 'assistant' ? 'model' : 'user',
  Â  Â  Â  Â  parts: [{ text: m.content }]
  Â  Â  Â  }));
  
  Â  Â  // Handle system messages by prepending to first user message
  Â  Â  const systemMessage = messages.find(m => m.role === 'system');
  Â  Â  if (systemMessage && contents.length > 0) {
  Â  Â  Â  contents[0].parts[0].text = `${systemMessage.content}\n\n${contents[0].parts[0].text}`;
  Â  Â  }
  
  Â  Â  const res = await fetch(
  Â  Â  Â  `${this.baseUrl}/models/${model}:generateContent?key=${this.apiKey}`,
  Â  Â  Â  {
  Â  Â  Â  Â  method: "POST",
  Â  Â  Â  Â  headers: { "Content-Type": "application/json" },
  Â  Â  Â  Â  body: JSON.stringify({
  Â  Â  Â  Â  Â  contents: contents,
  Â  Â  Â  Â  Â  generationConfig: {
  Â  Â  Â  Â  Â  Â  maxOutputTokens: 1000,
  Â  Â  Â  Â  Â  Â  temperature: 0.7,
  Â  Â  Â  Â  Â  }
  Â  Â  Â  Â  }),
  Â  Â  Â  }
  Â  Â  );
  
  Â  Â  if (!res.ok) {
  Â  Â  Â  const errorText = await res.text();
  Â  Â  Â  throw new Error(`Gemini API error: ${res.status} ${res.statusText} - ${errorText}`);
  Â  Â  }
  
  Â  Â  const data = await res.json();
  
  Â  Â  return {
  Â  Â  Â  content: data.candidates?.[0]?.content?.parts?.[0]?.text || "No response",
  Â  Â  Â  model,
  Â  Â  };
  Â  }
  }
  
  /* ------------------ OpenRouter Service (FREE MODELS) ------------------ */
  class OpenRouterService {
  Â  private apiKey: string;
  Â  private baseUrl: string;
  
  Â  constructor(config: AIModelConfig) {
  Â  Â  this.apiKey = config.apiKey;
  Â  Â  this.baseUrl = config.baseUrl || 'https://openrouter.ai/api/v1';
  Â  }
  
  Â  async chat(messages: ChatMessage[], model: string): Promise<AIResponse> {
  Â  Â  const res = await fetch(`${this.baseUrl}/chat/completions`, {
  Â  Â  Â  method: "POST",
  Â  Â  Â  headers: {
  Â  Â  Â  Â  "Authorization": `Bearer ${this.apiKey}`,
  Â  Â  Â  Â  "Content-Type": "application/json",
  Â  Â  Â  Â  "HTTP-Referer": window.location.origin,
  Â  Â  Â  Â  "X-Title": "AI Learning Tool",
  Â  Â  Â  },
  Â  Â  Â  body: JSON.stringify({ 
  Â  Â  Â  Â  model, 
  Â  Â  Â  Â  messages,
  Â  Â  Â  Â  max_tokens: 1000,
  Â  Â  Â  Â  temperature: 0.7
  Â  Â  Â  }),
  Â  Â  });
  
  Â  Â  if (!res.ok) {
  Â  Â  Â  const errorText = await res.text();
  Â  Â  Â  throw new Error(`OpenRouter API error: ${res.status} ${res.statusText} - ${errorText}`);
  Â  Â  }
  
  Â  Â  const data = await res.json();
  
  Â  Â  return {
  Â  Â  Â  content: data.choices?.[0]?.message?.content || 'No response',
  Â  Â  Â  model,
  Â  Â  Â  usage: data.usage,
  Â  Â  };
  Â  }
  }
  
  /* ------------------ FREE MODEL PROVIDERS ------------------ */
  export interface ModelProvider {
  Â  id: string;
  Â  name: string;
  Â  provider: string;
  Â  requiresApiKey: boolean;
  Â  freeLimit?: string;
  Â  description: string;
  }
  
  export const availableModels: ModelProvider[] = [
  Â  // ðŸŸ¢ Google Gemini - Best free option
  Â  { 
  Â  Â  id: 'gemini-1.5-flash', 
  Â  Â  name: 'Gemini 1.5 Flash', 
  Â  Â  provider: 'Google', 
  Â  Â  requiresApiKey: true, 
  Â  Â  freeLimit: '15 req/min, 1500/day',
  Â  Â  description: 'Fast, excellent for most tasks'
  Â  },
  Â  // ðŸŸ¢ OpenRouter free models
  Â  { 
  Â  Â  id: 'deepseek/deepseek-chat', 
  Â  Â  name: 'DeepSeek Chat', 
  Â  Â  provider: 'OpenRouter', 
  Â  Â  requiresApiKey: true,
  Â  Â  freeLimit: '$1 credit',
  Â  Â  description: 'Great coding assistant'
  Â  },
  ];
  
  /* ------------------ FREE AI SERVICE ------------------ */
  export class AIService {
  Â  private services: Map<string, GeminiService | OpenRouterService> = new Map();
  
  Â  constructor() {
  Â  Â  this.initializeServices();
  Â  }
  
  Â  private initializeServices() {
  Â  Â  // ðŸ†“ Google Gemini (FREE - Best option)
  Â  Â  const googleKey = import.meta.env.VITE_GOOGLE_API_KEY;
  Â  Â  if (googleKey && googleKey !== 'your_google_gemini_api_key_here') {
  Â  Â  Â  const geminiService = new GeminiService({ apiKey: googleKey });
  Â  Â  Â  this.services.set('gemini-1.5-flash', geminiService);
  Â  Â  }
  
  Â  Â  // ðŸ†“ OpenRouter (FREE models + DeepSeek)
  Â  Â  const openrouterKey = import.meta.env.VITE_OPENROUTER_API_KEY;
  Â  Â  if (openrouterKey && openrouterKey !== 'your_openrouter_api_key_here') {
  Â  Â  Â  const openrouterService = new OpenRouterService({ apiKey: openrouterKey });
  Â  Â  Â  this.services.set('deepseek/deepseek-chat', openrouterService);
  Â  Â  }
  Â  }
  
  Â  setApiKey(provider: string, apiKey: string) {
  Â  Â  switch (provider) {
  Â  Â  Â  case 'Google':
  Â  Â  Â  Â  const geminiService = new GeminiService({ apiKey });
  Â  Â  Â  Â  this.services.set('gemini-1.5-flash', geminiService);
  Â  Â  Â  Â  break;
  Â  Â  Â  case 'OpenRouter':
  Â  Â  Â  Â  const openrouterService = new OpenRouterService({ apiKey });
  Â  Â  Â  Â  this.services.set('deepseek/deepseek-chat', openrouterService);
  Â  Â  Â  Â  break;
  Â  Â  }
  Â  }
  
  Â  async chat(modelId: string, messages: ChatMessage[]): Promise<AIResponse> {
  Â  Â  const service = this.services.get(modelId);
  Â  Â  if (!service) {
  Â  Â  Â  throw new Error(`No API key configured for model: ${modelId}`);
  Â  Â  }
  Â  Â  return await service.chat(messages, modelId);
  Â  }
  
  Â  async compareModels(modelIds: string[], prompt: string): Promise<Record<string, AIResponse | Error>> {
  Â  Â  const messages: ChatMessage[] = [{ role: 'user', content: prompt }];
  Â  Â  const results: Record<string, AIResponse | Error> = {};
  
  Â  Â  // Process with small delays to avoid rate limits
  Â  Â  for (const modelId of modelIds) {
  Â  Â  Â  try {
  Â  Â  Â  Â  const response = await this.chat(modelId, messages);
  Â  Â  Â  Â  results[modelId] = response;
  Â  Â  Â  Â  // Small delay between requests
  Â  Â  Â  Â  await new Promise(resolve => setTimeout(resolve, 500));
  Â  Â  Â  } catch (error) {
  Â  Â  Â  Â  results[modelId] = error as Error;
  Â  Â  Â  }
  Â  Â  }
  
  Â  Â  return results;
  Â  }
  
  Â  hasApiKey(modelId: string): boolean {
  Â  Â  return this.services.has(modelId);
  Â  }
  
  Â  // ðŸŽ¯ Get the best free model available
  Â  getBestFreeModel(): string | null {
  Â  Â  if (this.hasApiKey('gemini-1.5-flash')) return 'gemini-1.5-flash';
  Â  Â  if (this.hasApiKey('deepseek/deepseek-chat')) return 'deepseek/deepseek-chat';
  Â  Â  return null;
  Â  }
  
  Â  // ðŸŽ¯ Get all available free models
  Â  getAvailableFreeModels(): ModelProvider[] {
  Â  Â  return availableModels.filter(model => this.hasApiKey(model.id));
  Â  }
  }
  
  export const aiService = new AIService();